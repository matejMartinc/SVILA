import transformers.models.gemma3.modeling_gemma3
from transformers.models.gemma3.modeling_gemma3 import Gemma3CausalLMOutputWithPast
import torch
from typing import Optional, List, Union, Tuple
import torch.nn as nn
import torch.distributed as dist
from transformers.cache_utils import Cache
from transformers.utils import is_torchdynamo_compiling
from transformers.masking_utils import create_causal_mask
import logging

logger = logging.getLogger(__name__)


def replace_gemma3_forward():
    transformers.models.gemma3.modeling_gemma3.Gemma3ForConditionalGeneration.forward = gemma3_mixed_modality_forward


def gemma3_mixed_modality_forward(
    self,
    input_ids: torch.LongTensor = None,
    pixel_values: torch.FloatTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.LongTensor] = None,
    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,
    token_type_ids: Optional[torch.LongTensor] = None,
    cache_position: Optional[torch.LongTensor] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[torch.LongTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
    logits_to_keep: Union[int, torch.Tensor] = 0,
    **lm_kwargs,
) -> Union[Tuple, Gemma3CausalLMOutputWithPast]:

    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    is_training = token_type_ids is not None and labels is not None
    image_features = None  # Ensure image_features is defined

    # Replace image id with PAD if the image token if OOV, to avoid index-errors
    if input_ids is not None and self.config.image_token_index >= self.config.text_config.vocab_size:
        special_image_mask = input_ids == self.config.image_token_index
        llm_input_ids = input_ids.clone()
        llm_input_ids[special_image_mask] = 0
    else:
        llm_input_ids = input_ids

    if inputs_embeds is None:
        inputs_embeds = self.get_input_embeddings()(llm_input_ids)

    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(
            past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        )

    local_mismatch = False

    if pixel_values is not None:
        image_features = self.get_image_features(pixel_values).to(inputs_embeds.device)

        # This part remains the same
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_index, dtype=torch.long, device=inputs_embeds.device)
            )
        else:
            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)
            special_image_mask = special_image_mask.expand_as(inputs_embeds)

        if not is_torchdynamo_compiling():
            # Calculate the total number of placeholder tokens in the batch.
            # This is the number of True values in the original, un-expanded mask.
            num_placeholder_tokens = (input_ids == self.config.image_token_index).sum()

            # Calculate the total number of patch embeddings generated by the vision model.
            # This is batch_size * num_patches_per_image.
            num_patch_embeddings = image_features.shape[0] * image_features.shape[1]

            if num_placeholder_tokens != num_patch_embeddings:
                logger.warning(
                    f"Data inconsistency found on Rank {dist.get_rank() if dist.is_initialized() else 0}: "
                    f"Found {num_placeholder_tokens} placeholder tokens but the vision model produced {num_patch_embeddings} patch embeddings."
                )
                local_mismatch = True

    # The synchronization logic MUST remain outside the conditional `if pixel_values is not None` block
    # to prevent deadlocks.
    mismatch_tensor = torch.tensor(1.0 if local_mismatch else 0.0, device=inputs_embeds.device)
    if dist.is_available() and dist.is_initialized():
        dist.all_reduce(mismatch_tensor, op=dist.ReduceOp.SUM)

    # Check the synchronized result.
    if mismatch_tensor.item() > 0:
        if not dist.is_initialized() or dist.get_rank() == 0:
            logger.warning(
                "Data inconsistency detected on at least one rank. "
                "Skipping image embedding merge for this entire global batch to prevent a crash."
            )
    elif pixel_values is not None:
        # All ranks confirmed data is okay. Proceed with merge only if this rank has images.
        # Note: We now use the expanded special_image_mask to perform the actual scatter operation.
        image_features_full = torch.zeros_like(inputs_embeds, dtype=image_features.dtype)
        image_features_full = image_features_full.masked_scatter(special_image_mask, image_features.view(-1, image_features.shape[-1]))
        inputs_embeds = torch.where(special_image_mask, image_features_full, inputs_embeds)

    # mask out pad-token-ids in labels for BC
    if labels is not None and self.config.text_config.pad_token_id in labels:
        logger.warning_once(
            "`labels` contains `pad_token_id` which will be masked with `config.ignore_index`. "
            "You have to mask out `pad_token_id` when preparing `labels`, this behavior will be removed in v.4.46.",
        )
        labels = torch.where(input_ids == self.config.text_config.pad_token_id, self.config.ignore_index, labels)

    causal_mask = create_causal_mask(
        config=self.config,
        input_embeds=inputs_embeds,
        attention_mask=attention_mask,
        cache_position=cache_position,
        past_key_values=past_key_values)

    outputs = self.language_model(
        attention_mask=causal_mask,
        position_ids=position_ids,
        past_key_values=past_key_values,
        inputs_embeds=inputs_embeds,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
        cache_position=cache_position,
        logits_to_keep=logits_to_keep,
        **lm_kwargs,
    )

    hidden_states = outputs[0]
    slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
    logits = self.lm_head(hidden_states[:, slice_indices, :])

    loss = None
    if labels is not None:
        logits = logits.float()
        shift_logits = logits[..., :-1, :]
        shift_labels = labels[..., 1:]
        if attention_mask is not None:
            shift_attention_mask = attention_mask[:, -shift_logits.shape[1]:].to(logits.device)
            shift_logits = shift_logits[shift_attention_mask.to(logits.device) != 0].contiguous()
            shift_labels = shift_labels[shift_attention_mask.to(shift_labels.device) != 0].contiguous()
        else:
            shift_logits = shift_logits.contiguous()
            shift_labels = shift_labels.contiguous()

        loss_fct = nn.CrossEntropyLoss()
        flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)
        flat_labels = shift_labels.view(-1).to(shift_logits.device)
        loss = loss_fct(flat_logits, flat_labels)

    if not return_dict:
        output = (logits,) + outputs[1:]
        return (loss,) + output if loss is not None else output

    return Gemma3CausalLMOutputWithPast(
        loss=loss,
        logits=logits,
        past_key_values=outputs.past_key_values,
        hidden_states=outputs.hidden_states,
        attentions=outputs.attentions,
        image_hidden_states=image_features if pixel_values is not None else None,
    )